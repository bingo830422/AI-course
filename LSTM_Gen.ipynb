{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用LSTM實作文章產生器\n",
    "## 實作LSTM，目標是做一個文章產生器，我們希望機器可以不斷的根據前文猜測下一個「字母」(Letters)應該要下什麼，如此一來我只要給個開頭字母，LSTM就可以生成一篇文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import所需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建制完成了字母庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading text8.zip\n",
      "Found and verified ./text8.zip\n",
      "=====\n",
      "Data size 100000000 letters\n",
      "=====\n",
      "Train Dataset: size: 99999000 letters,\n",
      "  first 64: ons anarchists advocate social relations based upon voluntary as\n",
      "Validation Dataset: size: 1000 letters,\n",
      "  first 64:  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "LETTER_SIZE = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "FIRST_LETTER_ASCII = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def maybe_download(url,filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - FIRST_LETTER_ASCII + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + FIRST_LETTER_ASCII - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(\"Downloading text8.zip\")\n",
    "filename = maybe_download('http://mattmahoney.net/dc/text8.zip','./text8.zip', 31344016)\n",
    "\n",
    "print(\"=====\")\n",
    "text = read_data(filename)\n",
    "print('Data size %d letters' % len(text))\n",
    "\n",
    "print(\"=====\")\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('Train Dataset: size:',train_size,'letters,\\n  first 64:',train_text[:64])\n",
    "print('Validation Dataset: size:',valid_size,'letters,\\n  first 64:',valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](images/TensorflowTutorial.013.jpeg)\n",
    "1. 設計一個LSTM Model，它的Unrolling Number為3，Batch Size為2，\n",
    "2. 然後遇到的字串是\"abcde fghij klmno pqrst\"，接下來就開始產生每個Round要用的Data，產生的結果如上圖所示，你會發現產生的Data第0軸表示的是考慮unrolling需要取樣的資料，總共應該會有(Unrolling Number+1)筆，如上圖例，共有4筆，3筆當作輸入而3筆當作Labels，中間有2筆重疊使用\n",
    "3. 會保留最後一筆Data當作下一個回合的第一筆，這是為了不浪費使用每一個字母前後的組合。\n",
    "4. 第1軸則是餵入單一LSTM需要的資料，我們一次可以餵多組不相干的字母進去，如上圖例，Batch Size=2所以餵2個字母進去，那這些不相干的字母在取樣的時候，我們會盡量讓它平均分配在文字庫，才能確保彼此之間不相干，以增加LSTM的訓練效率和效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 產生Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** train_batches:\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "*** valid_batches:\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def rnn_batch_generator(text, batch_size, num_unrollings):\n",
    "    text_size = len(text)\n",
    "\n",
    "    ### initialization\n",
    "    segment = text_size // batch_size\n",
    "    cursors = [ offset * segment for offset in range(batch_size)]\n",
    "\n",
    "    batches = []\n",
    "    batch_initial = np.zeros(shape=(batch_size, LETTER_SIZE), dtype=np.float)\n",
    "    for i in range(batch_size):\n",
    "        cursor = cursors[i]\n",
    "        id_ = char2id(text[cursor])\n",
    "        batch_initial[i][id_] = 1.0\n",
    "\n",
    "        #move cursor\n",
    "        cursors[i] = (cursors[i] + 1) % text_size\n",
    "\n",
    "    batches.append(batch_initial) \n",
    "\n",
    "    ### generate loop\n",
    "    while True:\n",
    "        batches = [ batches[-1] ]\n",
    "        for _ in range(num_unrollings):\n",
    "            batch = np.zeros(shape=(batch_size, LETTER_SIZE), dtype=np.float)\n",
    "            for i in range(batch_size):\n",
    "                cursor = cursors[i]\n",
    "                id_ = char2id(text[cursor])\n",
    "                batch[i][id_] = 1.0\n",
    "\n",
    "                #move cursor\n",
    "                cursors[i] = (cursors[i] + 1) % text_size\n",
    "            batches.append(batch)\n",
    "\n",
    "        yield batches  # [last batch of previous batches] + [unrollings]\n",
    "\n",
    "\n",
    "# demonstrate generator\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "train_batches = rnn_batch_generator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = rnn_batch_generator(valid_text, 1, 1)\n",
    "\n",
    "print(\"*** train_batches:\")\n",
    "print(batches2string(next(train_batches)))\n",
    "print(batches2string(next(train_batches)))\n",
    "print(\"*** valid_batches:\")\n",
    "print(batches2string(next(valid_batches)))\n",
    "print(batches2string(next(valid_batches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義一下會使用到的函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, LETTER_SIZE], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始建制LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    def __init__(self,n_unrollings,n_memory,n_train_batch,learning_rate=1.0):\n",
    "        self.n_unrollings = n_unrollings\n",
    "        self.n_memory = n_memory\n",
    "\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.saved = None\n",
    "\n",
    "        self.graph = tf.Graph() # initialize new grap\n",
    "        self.build(learning_rate,n_train_batch) # building graph\n",
    "        self.sess = tf.Session(graph=self.graph) # create session by the graph \n",
    "\n",
    "    def build(self,learning_rate,n_train_batch):\n",
    "        with self.graph.as_default():\n",
    "            ### Input      \n",
    "            self.train_data = list()\n",
    "            for _ in range(self.n_unrollings + 1):\n",
    "                self.train_data.append(\n",
    "                    tf.placeholder(tf.float32, shape=[n_train_batch,LETTER_SIZE]))\n",
    "            self.train_inputs = self.train_data[:self.n_unrollings]\n",
    "            self.train_labels = self.train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "\n",
    "            ### Optimalization\n",
    "            # build neurel network structure and get their loss\n",
    "            self.y_, self.loss = self.structure( inputs=self.train_inputs,\n",
    "                                                 labels=self.train_labels,\n",
    "                                                 n_batch=n_train_batch,\n",
    "                                               )\n",
    "\n",
    "            # define training operation\n",
    "\n",
    "            self.optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "            # gradient clipping\n",
    "            gradients, v = zip(*self.optimizer.compute_gradients(self.loss)) # output gradients one by one\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # clip gradient\n",
    "            self.train_op = self.optimizer.apply_gradients(zip(gradients, v)) # apply clipped gradients\n",
    "\n",
    "\n",
    "            ### Sampling and validation eval: batch 1, no unrolling.\n",
    "            self.sample_input = tf.placeholder(tf.float32, shape=[1, LETTER_SIZE])\n",
    "\n",
    "            saved_sample_output = tf.Variable(tf.zeros([1, self.n_memory]))\n",
    "            saved_sample_state = tf.Variable(tf.zeros([1, self.n_memory]))\n",
    "            self.reset_sample_state = tf.group(     # reset sample state operator\n",
    "                saved_sample_output.assign(tf.zeros([1, self.n_memory])),\n",
    "                saved_sample_state.assign(tf.zeros([1, self.n_memory])))\n",
    "\n",
    "            sample_output, sample_state = self.lstm_cell(\n",
    "                self.sample_input, saved_sample_output, saved_sample_state)\n",
    "            with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                          saved_sample_state.assign(sample_state)]):\n",
    "                # use tf.control_dependencies to make sure \"saving\" before \"prediction\"\n",
    "\n",
    "                self.sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, \n",
    "                                                                  self.weights['classifier'], \n",
    "                                                                  self.biases['classifier']))\n",
    "\n",
    "            ### Initialization\n",
    "            self.init_op = tf.global_variables_initializer()  \n",
    "\n",
    "    def lstm_cell(self,i,o,state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        ## Build Input Gate\n",
    "        ix = self.weights['input_gate_i']\n",
    "        im = self.weights['input_gate_o']\n",
    "        ib = self.biases['input_gate']\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        ## Build Forget Gate\n",
    "        fx = self.weights['forget_gate_i']\n",
    "        fm = self.weights['forget_gate_o']\n",
    "        fb = self.biases['forget_gate']        \n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        ## Memory\n",
    "        cx = self.weights['memory_i']\n",
    "        cm = self.weights['memory_o']\n",
    "        cb = self.biases['memory']\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        ## Update State\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        ## Build Output Gate        \n",
    "        ox = self.weights['output_gate_i']\n",
    "        om = self.weights['output_gate_o']\n",
    "        ob = self.biases['output_gate']\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        ## Ouput\n",
    "        output = output_gate * tf.tanh(state)\n",
    "\n",
    "        return output, state\n",
    "\n",
    "    def structure(self,inputs,labels,n_batch):\n",
    "        ### Variable\n",
    "        if (not self.weights) or (not self.biases) or (not self.saved):\n",
    "            self.weights = {\n",
    "              'input_gate_i': tf.Variable(tf.truncated_normal([LETTER_SIZE,self.n_memory], -0.1, 0.1)),\n",
    "              'input_gate_o': tf.Variable(tf.truncated_normal([self.n_memory,self.n_memory], -0.1, 0.1)),\n",
    "              'forget_gate_i': tf.Variable(tf.truncated_normal([LETTER_SIZE,self.n_memory], -0.1, 0.1)),\n",
    "              'forget_gate_o': tf.Variable(tf.truncated_normal([self.n_memory,self.n_memory], -0.1, 0.1)),\n",
    "              'output_gate_i': tf.Variable(tf.truncated_normal([LETTER_SIZE,self.n_memory], -0.1, 0.1)),\n",
    "              'output_gate_o': tf.Variable(tf.truncated_normal([self.n_memory,self.n_memory], -0.1, 0.1)),\n",
    "              'memory_i': tf.Variable(tf.truncated_normal([LETTER_SIZE,self.n_memory], -0.1, 0.1)),\n",
    "              'memory_o': tf.Variable(tf.truncated_normal([self.n_memory,self.n_memory], -0.1, 0.1)),\n",
    "              'classifier': tf.Variable(tf.truncated_normal([self.n_memory, LETTER_SIZE], -0.1, 0.1)),\n",
    "\n",
    "            }\n",
    "            self.biases = {\n",
    "              'input_gate': tf.Variable(tf.zeros([1, self.n_memory])),\n",
    "              'forget_gate': tf.Variable(tf.zeros([1, self.n_memory])),\n",
    "              'output_gate': tf.Variable(tf.zeros([1, self.n_memory])),\n",
    "              'memory': tf.Variable(tf.zeros([1, self.n_memory])),\n",
    "              'classifier': tf.Variable(tf.zeros([LETTER_SIZE])),\n",
    "            }\n",
    "\n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([n_batch, self.n_memory]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([n_batch, self.n_memory]), trainable=False)\n",
    "\n",
    "        ### Structure\n",
    "        # Unrolled LSTM loop.\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for input_ in inputs:\n",
    "            output, state = self.lstm_cell(input_, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # State saving across unrollings.\n",
    "        with tf.control_dependencies([saved_output.assign(output),\n",
    "                                      saved_state.assign(state)]):\n",
    "            # use tf.control_dependencies to make sure \"saving\" before \"calculating loss\"\n",
    "\n",
    "            # Classifier\n",
    "            logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), \n",
    "                                     self.weights['classifier'], \n",
    "                                     self.biases['classifier'])\n",
    "            y_ = tf.nn.softmax(logits)\n",
    "            loss = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        labels=tf.concat(labels, 0), logits=logits))\n",
    "\n",
    "        return y_, loss\n",
    "\n",
    "\n",
    "    def initialize(self):\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.sess.run(self.init_op)\n",
    "\n",
    "    def online_fit(self,X):      \n",
    "        feed_dict = dict()\n",
    "        for i in range(self.n_unrollings + 1):\n",
    "            feed_dict[self.train_data[i]] = X[i]\n",
    "\n",
    "        _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)    \n",
    "        return loss\n",
    "\n",
    "    def perplexity(self,X):\n",
    "        sum_logprob = 0\n",
    "        sample_size = len(X)-1\n",
    "        batch_size = X[0].shape[0]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            self.sess.run(self.reset_sample_state)\n",
    "            for j in range(sample_size):\n",
    "                sample_input = np.reshape(X[j][i],newshape=(1,-1))\n",
    "                sample_label = np.reshape(X[j+1][i],newshape=(1,-1))\n",
    "                predictions = self.sess.run(self.sample_prediction,\n",
    "                                            feed_dict={self.sample_input: sample_input})\n",
    "                sum_logprob += logprob(predictions, sample_label)\n",
    "        perplexity = float(np.exp(sum_logprob / batch_size / sample_size))\n",
    "        return perplexity\n",
    "\n",
    "    def generate(self,c,len_generate):\n",
    "        feed = np.array([[1 if id2char(i)==c else 0 for i in range(LETTER_SIZE)]])\n",
    "        sentence = characters(feed)[0]\n",
    "        self.sess.run(self.reset_sample_state)\n",
    "        for _ in range(len_generate):\n",
    "            prediction = self.sess.run(self.sample_prediction,feed_dict={self.sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-e3e301320a3e>:143: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch 1/30: 50s loss = 1.8302, perplexity = 5.8589\n",
      "Epoch 2/30: 48s loss = 1.5382, perplexity = 6.0558\n",
      "Epoch 3/30: 45s loss = 1.4768, perplexity = 5.9043\n",
      "Epoch 4/30: 47s loss = 1.4427, perplexity = 5.5036\n",
      "Epoch 5/30: 50s loss = 1.4253, perplexity = 6.3160\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.6695\n",
      "Generate From 'a':   alchand in alchatte his stoop rate hip or aminned c\n",
      "Generate From 'h':   hole icq under council cells are deutber the north \n",
      "Generate From 'm':   mence after he speedates one nine one zero zero pie\n",
      "==========================================\n",
      "\n",
      "Epoch 6/30: 46s loss = 1.4098, perplexity = 5.7530\n",
      "Epoch 7/30: 50s loss = 1.3948, perplexity = 5.6939\n",
      "Epoch 8/30: 50s loss = 1.3894, perplexity = 5.9189\n",
      "Epoch 9/30: 48s loss = 1.3655, perplexity = 5.9706\n",
      "Epoch 10/30: 52s loss = 1.3858, perplexity = 5.6960\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.6790\n",
      "Generate From 'a':   achs rulee this lecturers hectorus results love two\n",
      "Generate From 'h':   has defeiver drage included battled one two zero th\n",
      "Generate From 'm':   mer predings in two zero zero four war is loving th\n",
      "==========================================\n",
      "\n",
      "Epoch 11/30: 49s loss = 1.3775, perplexity = 5.7462\n",
      "Epoch 12/30: 51s loss = 1.3795, perplexity = 6.2523\n",
      "Epoch 13/30: 56s loss = 1.3712, perplexity = 6.4290\n",
      "Epoch 14/30: 52s loss = 1.3715, perplexity = 7.0148\n",
      "Epoch 15/30: 48s loss = 1.3666, perplexity = 6.8951\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.7634\n",
      "Generate From 'a':   ates of the placed his sembo base into setting an u\n",
      "Generate From 'h':   hee is no mix is inside gult one four swa and subso\n",
      "Generate From 'm':   ment duff the british relativou up and the sell als\n",
      "==========================================\n",
      "\n",
      "Epoch 16/30: 47s loss = 1.3626, perplexity = 5.5165\n",
      "Epoch 17/30: 47s loss = 1.3673, perplexity = 5.5169\n",
      "Epoch 18/30: 45s loss = 1.3504, perplexity = 6.4630\n",
      "Epoch 19/30: 51s loss = 1.3631, perplexity = 6.2959\n",
      "Epoch 20/30: 58s loss = 1.3544, perplexity = 5.9911\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.7465\n",
      "Generate From 'a':   ar separated and freder brgams one of the one genos\n",
      "Generate From 'h':   hall north historian faving one eight one one eight\n",
      "Generate From 'm':   man who consider nequ since any lesa can hold one s\n",
      "==========================================\n",
      "\n",
      "Epoch 21/30: 49s loss = 1.3556, perplexity = 6.5150\n",
      "Epoch 22/30: 45s loss = 1.3509, perplexity = 7.1641\n",
      "Epoch 23/30: 48s loss = 1.3670, perplexity = 5.7017\n",
      "Epoch 24/30: 51s loss = 1.3594, perplexity = 6.3984\n",
      "Epoch 25/30: 49s loss = 1.3639, perplexity = 6.9669\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.6199\n",
      "Generate From 'a':   ative the reinjsessed for load david goldod remove \n",
      "Generate From 'h':   heak take only an constitution as custers in the sa\n",
      "Generate From 'm':   ming deciding reform was named that a church rulrin\n",
      "==========================================\n",
      "\n",
      "Epoch 26/30: 47s loss = 1.3526, perplexity = 6.5237\n",
      "Epoch 27/30: 46s loss = 1.3561, perplexity = 6.4102\n",
      "Epoch 28/30: 50s loss = 1.3712, perplexity = 5.5143\n",
      "Epoch 29/30: 48s loss = 1.3610, perplexity = 6.3984\n",
      "Epoch 30/30: 50s loss = 1.3652, perplexity = 6.4789\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.4380\n",
      "Generate From 'a':   ar xp i maxuph of the mathematic by am from this fr\n",
      "Generate From 'h':   ha or realimment drives often could be clauded depr\n",
      "Generate From 'm':   man of american fieff as vesioper to each groups co\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build training batch generator\n",
    "batch_generator = rnn_batch_generator(text=train_text,\n",
    "                                      batch_size=batch_size,\n",
    "                                      num_unrollings=num_unrollings)\n",
    "\n",
    "# build validation data\n",
    "valid_batches = rnn_batch_generator(text=valid_text, \n",
    "                                    batch_size=1, \n",
    "                                    num_unrollings=1)\n",
    "\n",
    "valid_data = [np.array(next(valid_batches)) for _ in range(valid_size)]\n",
    "\n",
    "# build LSTM model\n",
    "model_LSTM = LSTM(n_unrollings=num_unrollings,\n",
    "                  n_memory=128,\n",
    "                  n_train_batch=batch_size,\n",
    "                  learning_rate=0.9)\n",
    "# initial model\n",
    "model_LSTM.initialize()\n",
    "\n",
    "# online training\n",
    "epochs = 30\n",
    "num_batchs_in_epoch = 5000\n",
    "valid_freq = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    avg_loss = 0\n",
    "    for _ in range(num_batchs_in_epoch):\n",
    "        batch = next(batch_generator)\n",
    "        loss = model_LSTM.online_fit(X=batch)\n",
    "        avg_loss += loss\n",
    "\n",
    "    avg_loss = avg_loss / num_batchs_in_epoch\n",
    "\n",
    "    train_perplexity = model_LSTM.perplexity(batch)\n",
    "    print(\"Epoch %d/%d: %ds loss = %6.4f, perplexity = %6.4f\" % ( epoch+1, epochs, time.time()-start_time,\n",
    "                                                   avg_loss, train_perplexity))\n",
    "\n",
    "    if (epoch+1) % valid_freq == 0:\n",
    "        print(\"\")\n",
    "        print(\"=============== Validation ===============\")\n",
    "        print(\"validation perplexity = %6.4f\" % (model_LSTM.perplexity(valid_data)))\n",
    "        print(\"Generate From 'a':  \",model_LSTM.generate(c='a',len_generate=50))\n",
    "        print(\"Generate From 'h':  \",model_LSTM.generate(c='h',len_generate=50))\n",
    "        print(\"Generate From 'm':  \",model_LSTM.generate(c='m',len_generate=50))\n",
    "        print(\"==========================================\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 產生一篇以\"p\"為開頭的1000字文章吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per prove to the outzuple and nasiff elevators thoms the rule hit in most notable in the infeuming fo superjader collain father writers in new ywany leba time of black in even discuept stamphile ftropically as an era allikiian control he real times and goundeston in two zero zero five third iii waye of the two a s construction of a lets in the terms this channele his join had his often hirgended he also switched that politician top xpyct and through a loyodomeh study of volume is frienming mammals plant another any device from man to bovest body without cellsgi fraced surfact of the aidle with new york wayshipways to this medical applicable senrification clement ilentary bravor haschlel contribution place one nine two zero hearning earth through the definition of after in violents part of not other diffected with the word the humal particularly differences literaturer internal retail from one prsit celectly access leon englanding one nine nine seven doctor open may roads from the grecos\n"
     ]
    }
   ],
   "source": [
    "#print(model_LSTM.generate(c='t',len_generate=1000))\n",
    "print(model_LSTM.generate(c='p',len_generate=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
