{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"word2vec.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ado171xZ0TxI","colab_type":"text"},"source":["# 實現 Word2Vec"]},{"cell_type":"code","metadata":{"id":"HucSXjlZ0TxL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":62},"outputId":"bcacd93e-a666-4789-f209-b218aded5214","executionInfo":{"status":"ok","timestamp":1576719753062,"user_tz":-480,"elapsed":3682,"user":{"displayName":"彭鈺翔","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB5xrjQaR36Xg2PqZsaoF3fh9uB2sujvr4kim7scA=s64","userId":"15879291729514508856"}}},"source":["from __future__ import division\n","import collections\n","import math\n","import os\n","import random\n","import zipfile\n","import numpy as np\n","import tensorflow as tf"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"nnz1S1NB0TxO","colab_type":"text"},"source":["使用urllib.urlretrieve下載資料的壓縮檔案，並核對檔案大小，若已經下載則可跳過。"]},{"cell_type":"code","metadata":{"id":"WUUzvaPP0TxP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"586472ef-42db-404b-ee22-535802dafb7e","executionInfo":{"status":"ok","timestamp":1576719868271,"user_tz":-480,"elapsed":118877,"user":{"displayName":"彭鈺翔","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB5xrjQaR36Xg2PqZsaoF3fh9uB2sujvr4kim7scA=s64","userId":"15879291729514508856"}}},"source":["from urllib.request import urlretrieve\n","def maybe_download(filename, expected_bytes):\n","    if not os.path.exists(filename):\n","        url =  \"http://mattmahoney.net/dc/\"\n","        filename, _ = urlretrieve(url + filename, filename)\n","    statinfo = os.stat(filename)\n","    if statinfo.st_size == expected_bytes:\n","        print('Found and verified', filename)\n","    else:\n","        print(statinfo.st_size)\n","        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n","    return filename\n","\n","filename = maybe_download('text8.zip', 31344016)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Found and verified text8.zip\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d47R4-3B0TxQ","colab_type":"text"},"source":["在瀏覽器输入 http://mattmahoney.net/dc/text8.zip 下载資料集。\n","\n","接下来解壓縮資料集，並使用 tf.compat.as_str 將資料集轉乘單詞的列表。"]},{"cell_type":"code","metadata":{"id":"ilyh7P9g0TxR","colab_type":"code","colab":{},"outputId":"4c3f559a-e022-4ee8-c068-d0b58372a9d7"},"source":["# 將詞存入 word 列表中\n","def read_data(filename):\n","    with zipfile.ZipFile(filename) as f:\n","        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n","    return data\n","\n","words = read_data(filename)\n","print ('Data size', len(words))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Data size 17005207\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l1JdiSud0TxU","colab_type":"text"},"source":["通過Output知道資料集最後被轉換成一個包含 17005207 個單詞的列表。"]},{"cell_type":"code","metadata":{"id":"rRA-2_Yc0TxU","colab_type":"code","colab":{}},"source":["vocabulary_size = 50000  # 將出现频率最高的 50000 個單詞放入 count 列表中，然後放入 dictionary 中\n","\n","\n","def build_dataset(words):\n","    count = [['UNK', -1]]  # 前面是詞彙，最後是出现的次数，這裡的 -1 在下面會填上 UNK 出现的頻率數\n","    # 將出现頻率最高的 50000 個詞存入count\n","    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))  # -1 因為 UNK 已经佔了一個了\n","\n","    dictionary = dict()\n","    for word, _ in count:\n","        dictionary[word] = len(dictionary)\n","   \n","    # Encoding：如果不出现在 dictionary 中，就以 0 作為編號，否则以 dictionary 中的編號為主   \n","    # 也就是將 words 中的所有詞的編號存在 data 中，並查一下 UNK 有多少，以便替换 count 中的 -1\n","    data = list()\n","    unk_count = 0\n","    for word in words:\n","        if word in dictionary:\n","            index = dictionary[word]\n","        else:\n","            index = 0\n","            unk_count += 1\n","        data.append(index)\n","\n","    count[0][1] = unk_count\n","\n","    # 編號：詞\n","    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n","    return data, count, dictionary, reverse_dictionary\n","\n","\n","data, count, dictionary, reverse_dictionary = build_dataset(words)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBSLMQP10TxW","colab_type":"text"},"source":["words[:10]\n","\n","输出：\n","['anarchism',\n"," 'originated',\n"," 'as',\n"," 'a',\n"," 'term',\n"," 'of',\n"," 'abuse',\n"," 'first',\n"," 'used',\n"," 'against']"]},{"cell_type":"markdown","metadata":{"id":"2uEouH2g0TxX","colab_type":"text"},"source":["data[:10] \n","\n","输出：\n","[5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]"]},{"cell_type":"markdown","metadata":{"id":"vaLq5dgM0TxX","colab_type":"text"},"source":["count[:10]\n","\n","输出：\n","[['UNK', 418391],\n"," ('the', 1061396),\n"," ('of', 593677),\n"," ('and', 416629),\n"," ('one', 411764),\n"," ('in', 372201),\n"," ('a', 325873),\n"," ('to', 316376),\n"," ('zero', 264975),\n"," ('nine', 250430)]\n","\n","dictionary # 词：编号\n","\n","输出：\n","{'fawn': 45848,\n"," 'homomorphism': 9648,\n"," 'nordisk': 39343,\n"," 'nunnery': 36075,\n"," 'chthonic': 33554,\n"," 'sowell': 40562,\n"," 'sonja': 38175,\n"," 'showa': 32906,\n"," 'woods': 6263,\n"," 'hsv': 44222,\n"," 'spiders': 14623,\n"," 'hanging': 8021,\n"," 'woody': 11150,\n"," ...\n","}"]},{"cell_type":"markdown","metadata":{"id":"upwPmuAF0TxY","colab_type":"text"},"source":["dictionary['UNK']\n","\n","输出：\n","0"]},{"cell_type":"markdown","metadata":{"id":"mWqpbYPk0TxY","colab_type":"text"},"source":["dictionary['a']\n","\n","输出：\n","6"]},{"cell_type":"markdown","metadata":{"id":"dib1wqJZ0TxZ","colab_type":"text"},"source":["reverse_dictionary # 编号：词\n","\n","输出：\n","{0: 'UNK',\n"," 1: 'the',\n"," 2: 'of',\n"," 3: 'and',\n"," 4: 'one',\n"," 5: 'in',\n"," 6: 'a',\n"," 7: 'to',\n"," 8: 'zero',\n"," ...\n","}"]},{"cell_type":"code","metadata":{"id":"ic-f70hk0TxZ","colab_type":"code","colab":{},"outputId":"01d00258-d177-48d7-af1c-107c8aa7ff26"},"source":["del words  # 刪除原始的單詞表，節省空間的浪費\n","print ('Most common word (+UNK)', count[:5])  # 列出最高頻率的詞彙\n","print ('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]]) # 前10個單詞編碼與單詞"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Most common word (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n","Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Tg980gN0Txb","colab_type":"code","colab":{}},"source":["# 生成 Word2Vec 訓練樣本\n","data_index = 0\n","\n","\n","def generate_batch(batch_size, num_skips, skip_window):\n","    global data_index  \n","    assert batch_size % num_skips == 0\n","    assert num_skips <= 2 * skip_window\n","\n","    # 將 batch 和 labels 初始化\n","    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n","    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n","\n","    # 對某個單詞創建相關樣本時會使用到的單詞數量，包括目標單詞本身和它前後的單詞\n","    span = 2 * skip_window + 1\n","\n","    # 創建最大容量為 span 的 deque（雙向隊列）\n","    # 在用 append 對 deque 添加變量時，只會保留最後插入的 span 個變量\n","    buffer = collections.deque(maxlen=span)\n","\n","    # 從 data_index 開始，把 span 個單詞順序讀入 buffer 作為初始值，buffer 中存的是詞的編號\n","    for _ in range(span):\n","        buffer.append(data[data_index])\n","        data_index = (data_index + 1) % len(data)\n","    # buffer 容量是 span，所以此時 buffer 已經填滿，後續的數據將替換掉前面的數據\n","\n","    # 每次循環內對一個目標單詞生成樣本，前方已經斷言能整除，這裡使用 // 是為了保證結果是 int\n","    for i in range(batch_size // num_skips):  # //除法只保留結果整數部分（python3中），python2中直接 /\n","        # 現在 buffer 中是目標單詞和所有相關單詞\n","        target = skip_window  # buffer 中第 skip_window 個單詞為目標單詞（注意第一個目標單詞是 buffer[skip_window]，並不是 buffer[0]）\n","        targets_to_avoid = [skip_window]  # 接下來生成相關（上下文語境）單詞，應將目標單詞拒絕\n","\n","        # 每次循環對一個語境單詞生成樣本\n","        for j in range(num_skips):\n","            # 先產生一個隨機數，直到隨機數不在 targets_to_avoid 中，就可以將之作為語境單詞\n","            while target in targets_to_avoid:\n","                target = random.randint(0, span - 1)\n","            targets_to_avoid.append(target)  # 因為這個語境單詞被使用了，所以要加入到 targets_to_avoid\n","\n","            batch[i * num_skips + j] = buffer[skip_window]  # feature 是目標詞彙\n","            labels[i * num_skips + j, 0] = buffer[target]  # label 是 buffer[target]\n","\n","        buffer.append(data[data_index])\n","        data_index = (data_index + 1) % len(data)\n","    return batch, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8jFdNLHg0Txd","colab_type":"text"},"source":["調用 generate_batch 函數測試一下功能。"]},{"cell_type":"code","metadata":{"id":"iKCHYakK0Txd","colab_type":"code","colab":{},"outputId":"b0af6713-c4b2-4229-ab2f-c6bf58b328f1"},"source":["batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n","for i in range(8):\n","    print (batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["3081 originated -> 12 as\n","3081 originated -> 5234 anarchism\n","12 as -> 3081 originated\n","12 as -> 6 a\n","6 a -> 12 as\n","6 a -> 195 term\n","195 term -> 6 a\n","195 term -> 2 of\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1UTswdpM0Txf","colab_type":"code","colab":{}},"source":["# 訓練需要的參數\n","batch_size = 128\n","embedding_size = 128  # 將單詞轉為稠密向量的維度，一般是500~1000這個範圍內的值，這裡設為128\n","skip_window = 1  # 單詞間最遠可以聯繫到的距離\n","num_skips = 2   #對每個目標單詞提取的樣本數\n","\n","# 生成驗證數據，隨機抽取一些頻數最高的單詞，看向量空間上跟它們距離最近的單詞是否相關性比較高\n","valid_size = 16  # 抽取的驗證單詞數\n","valid_window = 100  # 驗證單詞只從頻數最高的 100 個單詞中抽取\n","valid_examples = np.random.choice(valid_window, valid_size, replace=False)  # 隨機抽取\n","num_sampled = 64  # 訓練時用來做負樣本的噪聲單詞的數量"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqkHuIzq0Txh","colab_type":"code","colab":{},"outputId":"1f200237-0e32-4e2b-f1a0-dbc21fcf6c5f"},"source":["graph = tf.Graph()\n","with graph.as_default():\n","    \n","    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n","    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n","    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)  # 將前面隨機產生的 valid_examples 轉為 TensorFlow 中的 constant\n","\n","    with tf.device('/cpu:0'): # 限定所有计算在 CPU 上\n","    #with tf.device('/gpu:0'):\n","        # 隨機生成所有單詞的詞向量 embeddings，單詞表大小 5000，向量維度 128\n","        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n","        # 查找 train_inputs 對應的向量 embed\n","        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n","\n","        # 使用 NCE Loss 作為訓練的優化目標\n","        nce_weights = tf.Variable(\n","            tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n","        nce_bias = tf.Variable(tf.zeros([vocabulary_size]))\n","\n","    # 使用 tf.nn.nce_loss 計算學習出的詞向量 embed 在訓練數據上的 loss，並使用 tf.reduce_mean 進行匯總\n","    loss = tf.reduce_mean(\n","        tf.nn.nce_loss(weights=nce_weights, biases=nce_bias, labels=train_labels, inputs=embed, num_sampled=num_sampled,\n","                       num_classes=vocabulary_size))\n","\n","    # 定義優化器為 SGD，且學習速率為 1.0\n","    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n","\n","    # 計算嵌入向量 embeddings 的 L2 範數 norm\n","    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n","    # 標準化\n","    normalized_embeddings = embeddings / norm\n","    # 查詢驗證單詞的嵌入向量，併計算驗證單詞的嵌入向量與詞彙表中所有單詞的相似性\n","    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n","    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n","\n","    # 初始化所有模型参数\n","    init = tf.global_variables_initializer()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-9-94de95864a89>:29: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n","Instructions for updating:\n","keep_dims is deprecated, use keepdims instead\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7bdY93cG0Txj","colab_type":"code","colab":{},"outputId":"b7cea4f5-61a7-4323-c2ed-54b93e730754"},"source":["num_steps = 10001\n","#num_steps = 100001\n","\n","with tf.Session(graph=graph) as session:\n","    init.run()\n","    print ('Initialized')\n","\n","    average_loss = 0\n","    for step in range(num_steps):\n","        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n","        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n","\n","        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n","        average_loss += loss_val\n","\n","        if step % 2000 == 0:\n","            if step > 0:\n","                average_loss /= 2000\n","            print ('Average loss at step {} : {}'.format(step, average_loss))\n","            average_loss = 0\n","\n","        if step % 10000 == 0:\n","            sim = similarity.eval()\n","            for i in range(valid_size):\n","                valid_word = reverse_dictionary[valid_examples[i]]\n","                top_k = 8\n","                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n","                log_str = 'Nearest to {} :'.format(valid_word)\n","\n","                for k in range(top_k):\n","                    close_word = reverse_dictionary[nearest[k]]\n","                    log_str = '{} {},'.format(log_str, close_word)\n","                print (log_str)\n","        final_embeddings = normalized_embeddings.eval()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Average loss at step 0 : 272.80242919921875\n","Nearest to after : pacifists, callimico, hellboy, fibrous, retroactively, kodak, prot, galois,\n","Nearest to will : lifespan, durability, novelty, philippi, respects, canning, vespasian, breda,\n","Nearest to one : glaukos, minesweeper, roch, infiltration, researchers, greatness, mistrust, undesired,\n","Nearest to four : homonyms, motherboard, humphries, stenella, buchan, cans, eriksson, myasthenia,\n","Nearest to are : star, xenophobic, drusus, voiceless, pup, freeze, deposing, refreshed,\n","Nearest to is : sam, toothed, censuses, capricornus, weaknesses, subclass, papen, unlicensed,\n","Nearest to see : godavari, falconer, endothermic, covenants, shiite, mendes, metamorphic, caused,\n","Nearest to five : horace, contributing, logical, dreamt, ubangi, solvent, resorts, nicolau,\n","Nearest to have : covariant, polymorphic, convoys, overthrowing, rd, masculists, papacy, funniest,\n","Nearest to on : transitioning, numerology, genomes, shuffle, meaningless, torres, christened, oddly,\n","Nearest to UNK : secondary, microfluidics, fundamentalist, son, beli, nebulous, coltrane, lusitanian,\n","Nearest to by : scientologists, qquad, cnet, atacama, pli, martians, metallurgy, fish,\n","Nearest to three : procter, niccolo, pynchon, foosball, coy, radical, story, bluntly,\n","Nearest to over : coens, antiprism, ashton, abide, kapp, redmond, agilent, bitten,\n","Nearest to such : fleshed, sargon, comparing, shambles, howie, fung, ushering, alleged,\n","Nearest to was : mombasa, semitism, expandable, oni, unity, caesarion, cardinal, brixton,\n","Average loss at step 2000 : 113.91083439254761\n","Average loss at step 4000 : 52.591502084732056\n","Average loss at step 6000 : 33.413475214719774\n","Average loss at step 8000 : 23.527652347803116\n","Average loss at step 10000 : 18.098573712706568\n","Nearest to after : in, saints, agave, architect, fao, robots, amo, gland,\n","Nearest to will : americium, durability, pottery, lifespan, vs, mathbf, nearby, psi,\n","Nearest to one : phi, two, gland, vs, cl, UNK, victoriae, agave,\n","Nearest to four : aarhus, vs, nine, cl, authorities, eight, five, phi,\n","Nearest to are : is, were, star, and, of, was, reginae, kernel,\n","Nearest to is : was, are, phi, and, reginae, in, by, as,\n","Nearest to see : UNK, mtv, pottery, caused, were, college, anticipated, belonging,\n","Nearest to five : nine, vs, six, reginae, homomorphism, tubing, seven, analogue,\n","Nearest to have : be, climate, causal, british, literature, beta, rd, room,\n","Nearest to on : in, of, deism, and, two, canaris, one, oddly,\n","Nearest to UNK : alpina, agave, one, and, phi, cl, the, asterism,\n","Nearest to by : and, in, UNK, is, with, as, asterism, transitive,\n","Nearest to three : victoriae, nine, one, two, zero, cc, tuna, pottery,\n","Nearest to over : mathbf, previous, duty, mines, amazing, boroughs, etruscan, practice,\n","Nearest to such : alleged, pre, speak, customary, participating, bombing, oath, arose,\n","Nearest to was : is, had, and, backs, but, phi, banana, in,\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IHyBgNcu0Txm","colab_type":"text"},"source":["下面定義一個用來可視化 Word2Vec 效果的函數。這裡 low_dim_embs 是降維到 2 維 的單詞的空間向量，我們將在圖表中展示每個單詞的位置。使用 plt.scatter 顯示散點圖（單詞的位置），並用 plt.annotate 展示單詞本身，同時，使用 plt.savefig 保存圖片到本地文件。"]},{"cell_type":"code","metadata":{"id":"q8ledwP70Txn","colab_type":"code","colab":{}},"source":["def plot_with_labels(low_dim_embs,labels,filename='tsne.png'):\n","    assert low_dim_embs.shape[0]>=len(labels),'More labels then embeddings'\n","    plt.figure(figsize=(18,18))\n","    for i,label in enumerate(labels):\n","        x,y=low_dim_embs[i,:]\n","        plt.scatter(x,y)\n","        plt.annotate(label,xy=(x,y),xytext=(5,2),textcoords='offset points',ha='right',va='bottom')\n","    plt.savefig(filename)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9DPA4NI60Txo","colab_type":"text"},"source":["我們使用 sklearn.manifold.TSNE 實現降維，這裡直接將原始的 128 維的嵌入向量降到 2 維，再用前面的 plot_with_labels 函數進行展示。這裡只展示詞頻最高的 100 個單詞的可視化結果。"]},{"cell_type":"code","metadata":{"id":"-RHQPeXE0Txp","colab_type":"code","colab":{}},"source":["from sklearn.manifold import TSNE\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","tsne=TSNE(perplexity=30,n_components=2,init='pca',n_iter=5000)\n","plot_only=100\n","low_dim_embs=tsne.fit_transform(final_embeddings[:plot_only,:])\n","labels=[reverse_dictionary[i] for i in range(plot_only)]\n","plot_with_labels(low_dim_embs,labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oNOAIu8s0Txr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}